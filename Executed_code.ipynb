{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "#Allocation of memory for the GPU process\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "      [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3072)])\n",
    "logical = tf.config.experimental.list_logical_devices('GPU')\n",
    "print(logical[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 70\n",
    "\n",
    "# Training\n",
    "batch_size = 30\n",
    "epochs = 4 #modify as required, should be even\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "#Data split for unsupervised training\n",
    "x_train1 = x_train[0:2000]\n",
    "x_train2 = x_train[2001:24999]\n",
    "y_train1 = y_train[0:2000]\n",
    "y_train2 = y_train[2001:24999]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build models...\n",
      "Train Benchmark\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      " - 45s - loss: 0.3863 - accuracy: 0.8194 - val_loss: 0.3234 - val_accuracy: 0.8583\n",
      "Epoch 2/4\n",
      " - 43s - loss: 0.1957 - accuracy: 0.9244 - val_loss: 0.3376 - val_accuracy: 0.8567\n",
      "Epoch 3/4\n",
      " - 43s - loss: 0.0916 - accuracy: 0.9677 - val_loss: 0.5042 - val_accuracy: 0.8435\n",
      "Epoch 4/4\n",
      " - 45s - loss: 0.0405 - accuracy: 0.9875 - val_loss: 0.5013 - val_accuracy: 0.8404\n",
      "25000/25000 [==============================] - 8s 304us/step\n",
      "Benchmark test score: 0.6932262471914291\n",
      "Benchmark test accuracy: 0.5003600120544434\n",
      "Train on smaller set\n",
      "Train on 2000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      " - 11s - loss: 0.6740 - accuracy: 0.5710 - val_loss: 0.6226 - val_accuracy: 0.7008\n",
      "Epoch 2/4\n",
      " - 10s - loss: 0.3728 - accuracy: 0.8430 - val_loss: 0.4354 - val_accuracy: 0.7946\n",
      "Epoch 3/4\n",
      " - 10s - loss: 0.0867 - accuracy: 0.9725 - val_loss: 0.5201 - val_accuracy: 0.7906\n",
      "Epoch 4/4\n",
      " - 10s - loss: 0.0167 - accuracy: 0.9975 - val_loss: 0.6723 - val_accuracy: 0.7860\n",
      "25000/25000 [==============================] - 7s 297us/step\n",
      "Small model test score: 0.6722863692492247\n",
      "Small model test accuracy: 0.7860000133514404\n",
      "Train iter model\n",
      "Train on 2000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 11s - loss: 0.6813 - accuracy: 0.5625 - val_loss: 0.5936 - val_accuracy: 0.6648\n",
      "Epoch 2/2\n",
      " - 11s - loss: 0.3449 - accuracy: 0.8480 - val_loss: 0.4492 - val_accuracy: 0.7882\n",
      "Train on 11888 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 24s - loss: 0.0451 - accuracy: 0.9873 - val_loss: 1.0437 - val_accuracy: 0.7175\n",
      "Epoch 2/2\n",
      " - 24s - loss: 0.0172 - accuracy: 0.9951 - val_loss: 1.0956 - val_accuracy: 0.6972\n",
      "Iterative test score: 1.09557549456954\n",
      "Iterative test accuracy: 0.6972399950027466\n"
     ]
    }
   ],
   "source": [
    "print('Build models...')\n",
    "\n",
    "class Network(Sequential):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "    self.add(Dropout(0.25))\n",
    "    self.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    self.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    self.add(LSTM(lstm_output_size))\n",
    "    self.add(Dense(1))\n",
    "    self.add(Activation('sigmoid'))\n",
    "\n",
    "#Default for starting weights\n",
    "model = Network()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_benchmark = Network()\n",
    "model_small_set = Network()\n",
    "model_iterative_training = Network()\n",
    "\n",
    "#Benchmark\n",
    "model_benchmark.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_benchmark.set_weights(model.get_weights())\n",
    "\n",
    "print('Train Benchmark')\n",
    "model_benchmark.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),verbose=2)\n",
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Benchmark test score:', score)\n",
    "print('Benchmark test accuracy:', acc)\n",
    "model_benchmark.save('Mdl_Bench.h5')\n",
    "\n",
    "#Smaller training set\n",
    "print('Train on smaller set')\n",
    "model_small_set.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_small_set.set_weights(model.get_weights())\n",
    "model_small_set.fit(x_train1, y_train1,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),verbose=2)\n",
    "score, acc = model_small_set.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Small model test score:', score)\n",
    "print('Small model test accuracy:', acc)\n",
    "model_small_set.save('Mdl_Small.h5')\n",
    "\n",
    "#Iterative_train\n",
    "print('Train iter model')\n",
    "model_iterative_training.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model_iterative_training.set_weights(model.get_weights())\n",
    "\n",
    "model_iterative_training.fit(x_train1, y_train1,\n",
    "          batch_size=batch_size,\n",
    "          epochs=int(epochs/2),\n",
    "          validation_data=(x_test, y_test),verbose=2)\n",
    "\n",
    "a = model_iterative_training.predict(x_train2)\n",
    "\n",
    "i = 0\n",
    "k = np.array([1])\n",
    "for x in np.nditer(a):\n",
    "     if x>=0.5:\n",
    "        x_train1 = np.append(x_train1, np.array([x_train2[i]]), axis = 0)\n",
    "        y_train1 = np.append(y_train1, k)\n",
    "        i += 1\n",
    "     else:\n",
    "        i += 1\n",
    "\n",
    "\n",
    "model_iterative_training.fit(x_train1, y_train1,\n",
    "          batch_size=batch_size,\n",
    "          epochs=int(epochs/2),\n",
    "          validation_data=(x_test, y_test),verbose=2)\n",
    "\n",
    "score, acc = model_iterative_training.evaluate(x_test, y_test, batch_size=batch_size, verbose =2)\n",
    "print('Iterative test score:', score)\n",
    "print('Iterative test accuracy:', acc)\n",
    "model_iterative_training.save('Mdl_iter.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
